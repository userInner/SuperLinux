"""
Phase 2: è‡ªæˆ‘è¯Šæ–­ç³»ç»Ÿ

è®© AI èƒ½å¤Ÿè‡ªåŠ¨è¯„ä¼°è‡ªå·±çš„è¡¨ç°å¹¶ç”Ÿæˆæ”¹è¿›å»ºè®®
"""

import json
import os
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from typing import Optional
from enum import Enum


class ImprovementType(Enum):
    """æ”¹è¿›ç±»å‹"""
    PROMPT = "prompt"
    TOOL = "tool"
    STRATEGY = "strategy"


class Priority(Enum):
    """ä¼˜å…ˆçº§"""
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"


@dataclass
class TaskEvaluation:
    """ä»»åŠ¡è¯„ä¼°ç»“æœ"""
    task_id: str
    task: str
    timestamp: str
    
    # è¯„ä¼°ç»´åº¦ (0-100)
    success_score: float
    efficiency_score: float
    tool_usage_score: float
    error_handling_score: float
    user_satisfaction_score: float
    
    # æ€»åˆ†
    overall_score: float
    
    # è¯„è¯­
    comments: list[str]
    
    # åŸå§‹æ•°æ®
    execution_time: float
    steps_count: int
    tools_used: list[str]
    errors_count: int


@dataclass
class ImprovementSuggestion:
    """æ”¹è¿›å»ºè®®"""
    id: str
    type: ImprovementType
    priority: Priority
    
    issue: str  # è¯†åˆ«çš„é—®é¢˜
    suggestion: str  # å…·ä½“å»ºè®®
    expected_improvement: str  # é¢„æœŸæ•ˆæœ
    
    # ç›¸å…³æ•°æ®
    affected_area: str  # å½±å“çš„é¢†åŸŸ
    evidence: list[str]  # è¯æ®
    
    timestamp: str
    status: str = "pending"  # pending/applied/rejected


@dataclass
class MetaExperience:
    """å…ƒç»éªŒ - å…³äºè‡ªæˆ‘æ”¹è¿›çš„ç»éªŒ"""
    id: str
    improvement_type: str
    
    problem_identified: str
    solution_applied: str
    
    # æ”¹è¿›å‰åçš„æŒ‡æ ‡
    before_metrics: dict
    after_metrics: dict
    
    # æœ‰æ•ˆæ€§ (0-1)
    effectiveness: float
    
    timestamp: str
    notes: str = ""
    
    def is_effective(self) -> bool:
        """åˆ¤æ–­æ”¹è¿›æ˜¯å¦æœ‰æ•ˆ"""
        return self.effectiveness > 0.2  # æå‡è¶…è¿‡ 20%


class TaskEvaluator:
    """ä»»åŠ¡è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.evaluation_history = []
    
    def evaluate_task(
        self,
        task: str,
        result: str,
        steps: list[str],
        tools_used: list[str],
        errors: list[str],
        execution_time: float,
        success: bool
    ) -> TaskEvaluation:
        """è¯„ä¼°å•ä¸ªä»»åŠ¡"""
        
        task_id = self._generate_id(task)
        
        # è¯„ä¼°å„ä¸ªç»´åº¦
        success_score = self._evaluate_success(success, errors)
        efficiency_score = self._evaluate_efficiency(steps, execution_time)
        tool_usage_score = self._evaluate_tool_usage(tools_used, task)
        error_handling_score = self._evaluate_error_handling(errors)
        user_satisfaction_score = self._estimate_satisfaction(result, success)
        
        # è®¡ç®—æ€»åˆ†ï¼ˆåŠ æƒå¹³å‡ï¼‰
        overall_score = (
            success_score * 0.35 +
            efficiency_score * 0.20 +
            tool_usage_score * 0.15 +
            error_handling_score * 0.15 +
            user_satisfaction_score * 0.15
        )
        
        # ç”Ÿæˆè¯„è¯­
        comments = self._generate_comments({
            "success": success_score,
            "efficiency": efficiency_score,
            "tool_usage": tool_usage_score,
            "error_handling": error_handling_score,
            "user_satisfaction": user_satisfaction_score
        })
        
        evaluation = TaskEvaluation(
            task_id=task_id,
            task=task[:200],
            timestamp=datetime.now().isoformat(),
            success_score=success_score,
            efficiency_score=efficiency_score,
            tool_usage_score=tool_usage_score,
            error_handling_score=error_handling_score,
            user_satisfaction_score=user_satisfaction_score,
            overall_score=overall_score,
            comments=comments,
            execution_time=execution_time,
            steps_count=len(steps),
            tools_used=tools_used,
            errors_count=len(errors)
        )
        
        self.evaluation_history.append(evaluation)
        return evaluation
    
    def _evaluate_success(self, success: bool, errors: list[str]) -> float:
        """è¯„ä¼°æˆåŠŸåº¦"""
        if success and not errors:
            return 100.0
        elif success and errors:
            return 80.0 - min(len(errors) * 10, 30)
        else:
            return max(20.0 - len(errors) * 5, 0)
    
    def _evaluate_efficiency(self, steps: list[str], execution_time: float) -> float:
        """è¯„ä¼°æ•ˆç‡"""
        # æ­¥éª¤æ•°è¯„åˆ†
        if len(steps) <= 3:
            step_score = 100
        elif len(steps) <= 5:
            step_score = 80
        elif len(steps) <= 8:
            step_score = 60
        else:
            step_score = max(40 - (len(steps) - 8) * 5, 20)
        
        # æ—¶é—´è¯„åˆ†
        if execution_time < 10:
            time_score = 100
        elif execution_time < 30:
            time_score = 80
        elif execution_time < 60:
            time_score = 60
        else:
            time_score = max(40 - (execution_time - 60) / 10, 20)
        
        return (step_score + time_score) / 2
    
    def _evaluate_tool_usage(self, tools_used: list[str], task: str) -> float:
        """è¯„ä¼°å·¥å…·ä½¿ç”¨"""
        if not tools_used:
            return 50.0
        
        # å·¥å…·å¤šæ ·æ€§
        unique_tools = len(set(tools_used))
        diversity_score = min(unique_tools * 20, 100)
        
        # å·¥å…·ç›¸å…³æ€§ï¼ˆç®€å•å¯å‘å¼ï¼‰
        task_lower = task.lower()
        relevant_tools = 0
        
        if "æ–‡ä»¶" in task_lower or "ä»£ç " in task_lower:
            if any(t in ["read_file", "write_file", "edit_file"] for t in tools_used):
                relevant_tools += 1
        
        if "æœç´¢" in task_lower or "æŸ¥æ‰¾" in task_lower:
            if any(t in ["web_search", "search_in_files"] for t in tools_used):
                relevant_tools += 1
        
        if "ç³»ç»Ÿ" in task_lower or "ç›‘æ§" in task_lower:
            if any(t in ["get_system_stats", "get_cpu_info"] for t in tools_used):
                relevant_tools += 1
        
        relevance_score = min(relevant_tools * 30, 100)
        
        return (diversity_score * 0.4 + relevance_score * 0.6)
    
    def _evaluate_error_handling(self, errors: list[str]) -> float:
        """è¯„ä¼°é”™è¯¯å¤„ç†"""
        if not errors:
            return 100.0
        
        # é”™è¯¯æ•°é‡è¶Šå°‘è¶Šå¥½
        error_count_score = max(100 - len(errors) * 15, 0)
        
        return error_count_score
    
    def _estimate_satisfaction(self, result: str, success: bool) -> float:
        """ä¼°ç®—ç”¨æˆ·æ»¡æ„åº¦"""
        if not success:
            return 30.0
        
        # åŸºäºç»“æœé•¿åº¦å’Œå†…å®¹çš„ç®€å•ä¼°ç®—
        if len(result) < 50:
            return 60.0
        elif len(result) < 200:
            return 80.0
        else:
            return 90.0
    
    def _generate_comments(self, dimensions: dict) -> list[str]:
        """ç”Ÿæˆè¯„è¯­"""
        comments = []
        
        if dimensions["success"] >= 90:
            comments.append("âœ… ä»»åŠ¡å®Œæˆåº¦ä¼˜ç§€")
        elif dimensions["success"] < 60:
            comments.append("âš ï¸ ä»»åŠ¡å®Œæˆåº¦éœ€è¦æ”¹è¿›")
        
        if dimensions["efficiency"] >= 80:
            comments.append("âš¡ æ‰§è¡Œæ•ˆç‡å¾ˆé«˜")
        elif dimensions["efficiency"] < 60:
            comments.append("ğŸŒ æ‰§è¡Œæ•ˆç‡åä½ï¼Œå¯ä»¥ä¼˜åŒ–")
        
        if dimensions["tool_usage"] < 60:
            comments.append("ğŸ”§ å·¥å…·ä½¿ç”¨å¯ä»¥æ›´åˆç†")
        
        if dimensions["error_handling"] < 70:
            comments.append("ğŸ› é”™è¯¯å¤„ç†éœ€è¦åŠ å¼º")
        
        return comments
    
    def _generate_id(self, task: str) -> str:
        """ç”Ÿæˆä»»åŠ¡ ID"""
        import hashlib
        content = f"{task}{datetime.now().isoformat()}"
        return hashlib.md5(content.encode()).hexdigest()[:12]


class SuggestionGenerator:
    """æ”¹è¿›å»ºè®®ç”Ÿæˆå™¨"""
    
    def __init__(self, db_path: str = "./experience_db"):
        self.db_path = db_path
        self.suggestions_file = os.path.join(db_path, "suggestions.json")
    
    def generate_suggestions(
        self,
        evaluations: list[TaskEvaluation],
        focus_area: str = "all",
        priority: str = "all"
    ) -> list[ImprovementSuggestion]:
        """åŸºäºè¯„ä¼°ç»“æœç”Ÿæˆæ”¹è¿›å»ºè®®"""
        
        suggestions = []
        
        # åˆ†æè¯„ä¼°æ•°æ®
        analysis = self._analyze_evaluations(evaluations)
        
        # ç”Ÿæˆä¸åŒç±»å‹çš„å»ºè®®
        if focus_area in ["prompt", "all"]:
            suggestions.extend(self._generate_prompt_suggestions(analysis))
        
        if focus_area in ["tool", "all"]:
            suggestions.extend(self._generate_tool_suggestions(analysis))
        
        if focus_area in ["strategy", "all"]:
            suggestions.extend(self._generate_strategy_suggestions(analysis))
        
        # è¿‡æ»¤ä¼˜å…ˆçº§
        if priority != "all":
            suggestions = [s for s in suggestions if s.priority.value == priority]
        
        # ä¿å­˜å»ºè®®
        self._save_suggestions(suggestions)
        
        return suggestions
    
    def _analyze_evaluations(self, evaluations: list[TaskEvaluation]) -> dict:
        """åˆ†æè¯„ä¼°æ•°æ®"""
        if not evaluations:
            return {}
        
        total = len(evaluations)
        
        return {
            "total_tasks": total,
            "avg_success": sum(e.success_score for e in evaluations) / total,
            "avg_efficiency": sum(e.efficiency_score for e in evaluations) / total,
            "avg_tool_usage": sum(e.tool_usage_score for e in evaluations) / total,
            "avg_error_handling": sum(e.error_handling_score for e in evaluations) / total,
            "total_errors": sum(e.errors_count for e in evaluations),
            "avg_execution_time": sum(e.execution_time for e in evaluations) / total,
            "all_tools_used": [tool for e in evaluations for tool in e.tools_used]
        }
    
    def _generate_prompt_suggestions(self, analysis: dict) -> list[ImprovementSuggestion]:
        """ç”Ÿæˆ Prompt ä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        # æˆåŠŸç‡ä½
        if analysis.get("avg_success", 100) < 70:
            suggestions.append(ImprovementSuggestion(
                id=self._generate_id("prompt_success"),
                type=ImprovementType.PROMPT,
                priority=Priority.HIGH,
                issue=f"ä»»åŠ¡æˆåŠŸç‡åä½ ({analysis['avg_success']:.1f}%)",
                suggestion="åœ¨ SYSTEM_PROMPT ä¸­å¼ºåŒ–'å…ˆæ¢ç´¢åè¡ŒåŠ¨'åŸåˆ™ï¼Œè¦æ±‚ AI åœ¨ä¸ç¡®å®šæ—¶å…ˆä½¿ç”¨å·¥å…·æ”¶é›†ä¿¡æ¯",
                expected_improvement="é¢„è®¡æå‡æˆåŠŸç‡ 15-20%",
                affected_area="prompts.py - SYSTEM_PROMPT_V2",
                evidence=[
                    f"æœ€è¿‘ {analysis['total_tasks']} ä¸ªä»»åŠ¡çš„å¹³å‡æˆåŠŸç‡: {analysis['avg_success']:.1f}%",
                    "å»ºè®®æ·»åŠ æ›´å¤šé”™è¯¯å¤„ç†æŒ‡å¯¼"
                ],
                timestamp=datetime.now().isoformat()
            ))
        
        # æ•ˆç‡ä½
        if analysis.get("avg_efficiency", 100) < 60:
            suggestions.append(ImprovementSuggestion(
                id=self._generate_id("prompt_efficiency"),
                type=ImprovementType.PROMPT,
                priority=Priority.MEDIUM,
                issue=f"æ‰§è¡Œæ•ˆç‡åä½ ({analysis['avg_efficiency']:.1f}åˆ†)",
                suggestion="åœ¨ Prompt ä¸­æ·»åŠ 'ä¸€æ¬¡æ€§è·å–è¶³å¤Ÿä¿¡æ¯'çš„æŒ‡å¯¼ï¼Œå‡å°‘é‡å¤å·¥å…·è°ƒç”¨",
                expected_improvement="é¢„è®¡å‡å°‘ 30% çš„æ‰§è¡Œæ—¶é—´",
                affected_area="prompts.py - å·¥å…·ä½¿ç”¨ç­–ç•¥",
                evidence=[
                    f"å¹³å‡æ‰§è¡Œæ—¶é—´: {analysis.get('avg_execution_time', 0):.1f}ç§’",
                    "å»ºè®®ä¼˜åŒ–å·¥å…·è°ƒç”¨ç­–ç•¥"
                ],
                timestamp=datetime.now().isoformat()
            ))
        
        return suggestions
    
    def _generate_tool_suggestions(self, analysis: dict) -> list[ImprovementSuggestion]:
        """ç”Ÿæˆå·¥å…·æ”¹è¿›å»ºè®®"""
        suggestions = []
        
        # å·¥å…·ä½¿ç”¨å•ä¸€
        unique_tools = len(set(analysis.get("all_tools_used", [])))
        if unique_tools < 5:
            suggestions.append(ImprovementSuggestion(
                id=self._generate_id("tool_diversity"),
                type=ImprovementType.TOOL,
                priority=Priority.MEDIUM,
                issue=f"å·¥å…·ä½¿ç”¨ç§ç±»è¾ƒå°‘ (åªç”¨äº† {unique_tools} ç§å·¥å…·)",
                suggestion="æ‰©å±•å·¥å…·é›†ï¼Œæ·»åŠ æ›´å¤šä¸“ç”¨å·¥å…·ï¼ˆå¦‚ search_in_files, analyze_trends ç­‰ï¼‰",
                expected_improvement="æé«˜é—®é¢˜è§£å†³èƒ½åŠ› 25%",
                affected_area="tools.py - å·¥å…·å®šä¹‰",
                evidence=[
                    f"å¸¸ç”¨å·¥å…·: {list(set(analysis.get('all_tools_used', [])))[:5]}",
                    "å»ºè®®æ·»åŠ æ›´å¤šä¸“ç”¨å·¥å…·"
                ],
                timestamp=datetime.now().isoformat()
            ))
        
        return suggestions
    
    def _generate_strategy_suggestions(self, analysis: dict) -> list[ImprovementSuggestion]:
        """ç”Ÿæˆç­–ç•¥è°ƒæ•´å»ºè®®"""
        suggestions = []
        
        # é”™è¯¯å¤„ç†å·®
        if analysis.get("avg_error_handling", 100) < 60:
            suggestions.append(ImprovementSuggestion(
                id=self._generate_id("strategy_error"),
                type=ImprovementType.STRATEGY,
                priority=Priority.HIGH,
                issue=f"é”™è¯¯å¤„ç†èƒ½åŠ›ä¸è¶³ (æ€»é”™è¯¯æ•°: {analysis.get('total_errors', 0)})",
                suggestion="å®æ–½'ä¸‰æ¬¡é‡è¯•'ç­–ç•¥ï¼šé‡åˆ°é”™è¯¯æ—¶ï¼Œå…ˆåˆ†æåŸå› ï¼Œè°ƒæ•´æ–¹æ³•ï¼Œæœ€å¤šé‡è¯•3æ¬¡",
                expected_improvement="å‡å°‘ 40% çš„å¤±è´¥ä»»åŠ¡",
                affected_area="æ‰§è¡Œç­–ç•¥",
                evidence=[
                    f"æ€»é”™è¯¯æ•°: {analysis.get('total_errors', 0)}",
                    "å»ºè®®åŠ å¼ºé”™è¯¯æ¢å¤æœºåˆ¶"
                ],
                timestamp=datetime.now().isoformat()
            ))
        
        return suggestions
    
    def _save_suggestions(self, suggestions: list[ImprovementSuggestion]):
        """ä¿å­˜å»ºè®®åˆ°æ–‡ä»¶"""
        os.makedirs(self.db_path, exist_ok=True)
        
        # è¯»å–ç°æœ‰å»ºè®®
        existing = []
        if os.path.exists(self.suggestions_file):
            try:
                with open(self.suggestions_file, "r", encoding="utf-8") as f:
                    existing = json.load(f)
            except:
                existing = []
        
        # æ·»åŠ æ–°å»ºè®®ï¼ˆè½¬æ¢ Enum ä¸ºå­—ç¬¦ä¸²ï¼‰
        for suggestion in suggestions:
            suggestion_dict = asdict(suggestion)
            # è½¬æ¢ Enum ä¸ºå­—ç¬¦ä¸²
            suggestion_dict["type"] = suggestion.type.value
            suggestion_dict["priority"] = suggestion.priority.value
            existing.append(suggestion_dict)
        
        # åªä¿ç•™æœ€è¿‘ 100 æ¡
        if len(existing) > 100:
            existing = existing[-100:]
        
        # ä¿å­˜
        with open(self.suggestions_file, "w", encoding="utf-8") as f:
            json.dump(existing, f, ensure_ascii=False, indent=2)
    
    def _generate_id(self, prefix: str) -> str:
        """ç”Ÿæˆå»ºè®® ID"""
        import hashlib
        content = f"{prefix}{datetime.now().isoformat()}"
        return hashlib.md5(content.encode()).hexdigest()[:12]


class MetaExperienceManager:
    """å…ƒç»éªŒç®¡ç†å™¨"""
    
    def __init__(self, db_path: str = "./experience_db"):
        self.db_path = db_path
        self.meta_file = os.path.join(db_path, "meta_experiences.json")
    
    def record_improvement(
        self,
        improvement_type: str,
        problem: str,
        solution: str,
        before_metrics: dict,
        after_metrics: dict
    ) -> MetaExperience:
        """è®°å½•ä¸€æ¬¡æ”¹è¿›"""
        
        # è®¡ç®—æœ‰æ•ˆæ€§
        effectiveness = self._calculate_effectiveness(before_metrics, after_metrics)
        
        meta_exp = MetaExperience(
            id=self._generate_id(problem),
            improvement_type=improvement_type,
            problem_identified=problem,
            solution_applied=solution,
            before_metrics=before_metrics,
            after_metrics=after_metrics,
            effectiveness=effectiveness,
            timestamp=datetime.now().isoformat()
        )
        
        # ä¿å­˜
        self._save_meta_experience(meta_exp)
        
        return meta_exp
    
    def get_effective_improvements(
        self,
        improvement_type: str = "all",
        min_effectiveness: float = 0.2
    ) -> list[MetaExperience]:
        """è·å–æœ‰æ•ˆçš„æ”¹è¿›"""
        
        all_meta = self._load_meta_experiences()
        
        # è¿‡æ»¤
        filtered = [
            m for m in all_meta
            if m.effectiveness >= min_effectiveness
            and (improvement_type == "all" or m.improvement_type == improvement_type)
        ]
        
        # æŒ‰æœ‰æ•ˆæ€§æ’åº
        filtered.sort(key=lambda x: x.effectiveness, reverse=True)
        
        return filtered
    
    def _calculate_effectiveness(self, before: dict, after: dict) -> float:
        """è®¡ç®—æ”¹è¿›æœ‰æ•ˆæ€§"""
        if not before or not after:
            return 0.0
        
        # è®¡ç®—ä¸»è¦æŒ‡æ ‡çš„æ”¹è¿›
        improvements = []
        
        for key in ["success_rate", "efficiency", "error_rate"]:
            if key in before and key in after:
                before_val = before[key]
                after_val = after[key]
                
                if before_val > 0:
                    if key == "error_rate":
                        # é”™è¯¯ç‡é™ä½æ˜¯å¥½äº‹
                        improvement = (before_val - after_val) / before_val
                    else:
                        # å…¶ä»–æŒ‡æ ‡æå‡æ˜¯å¥½äº‹
                        improvement = (after_val - before_val) / before_val
                    
                    improvements.append(improvement)
        
        if not improvements:
            return 0.0
        
        # è¿”å›å¹³å‡æ”¹è¿›ç‡
        return sum(improvements) / len(improvements)
    
    def _save_meta_experience(self, meta_exp: MetaExperience):
        """ä¿å­˜å…ƒç»éªŒ"""
        os.makedirs(self.db_path, exist_ok=True)
        
        all_meta = self._load_meta_experiences()
        all_meta.append(meta_exp)
        
        # åªä¿ç•™æœ€è¿‘ 50 æ¡
        if len(all_meta) > 50:
            all_meta = all_meta[-50:]
        
        with open(self.meta_file, "w", encoding="utf-8") as f:
            json.dump(
                [asdict(m) for m in all_meta],
                f,
                ensure_ascii=False,
                indent=2
            )
    
    def _load_meta_experiences(self) -> list[MetaExperience]:
        """åŠ è½½å…ƒç»éªŒ"""
        if not os.path.exists(self.meta_file):
            return []
        
        try:
            with open(self.meta_file, "r", encoding="utf-8") as f:
                data = json.load(f)
            return [MetaExperience(**item) for item in data]
        except:
            return []
    
    def _generate_id(self, problem: str) -> str:
        """ç”Ÿæˆ ID"""
        import hashlib
        content = f"{problem}{datetime.now().isoformat()}"
        return hashlib.md5(content.encode()).hexdigest()[:12]


# å…¨å±€å®ä¾‹
_evaluator: Optional[TaskEvaluator] = None
_suggestion_generator: Optional[SuggestionGenerator] = None
_meta_manager: Optional[MetaExperienceManager] = None


def get_evaluator() -> TaskEvaluator:
    """è·å–å…¨å±€è¯„ä¼°å™¨"""
    global _evaluator
    if _evaluator is None:
        _evaluator = TaskEvaluator()
    return _evaluator


def get_suggestion_generator() -> SuggestionGenerator:
    """è·å–å…¨å±€å»ºè®®ç”Ÿæˆå™¨"""
    global _suggestion_generator
    if _suggestion_generator is None:
        _suggestion_generator = SuggestionGenerator()
    return _suggestion_generator


def get_meta_manager() -> MetaExperienceManager:
    """è·å–å…¨å±€å…ƒç»éªŒç®¡ç†å™¨"""
    global _meta_manager
    if _meta_manager is None:
        _meta_manager = MetaExperienceManager()
    return _meta_manager
